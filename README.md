

This is a simple MNIST recognizer I built from scratch using Python that has a single hidden layer I made just to learn the basic calculus behind backpropogation.

It got ~90% accuracy when it had 128 neurons in the hidden layer, and around ~85% when it had 10. 
I tested it out with some epochs and batch sizes so it wasn't just pure Gradient Descent but stochastic but I forgot to commit the code and rolled back a version on kaggle : | but I think I've gotten a good intuition for the thing overall so I'm not rewriting anything.

There's also a photo of my notebook with backprop math ( for 10 hidden neurons )
